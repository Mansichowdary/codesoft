import numpy as np 
import pandas as pd
# Setting the random seed for reproducibility 
np.random.seed(42)
# Number of records 
num_records = 5000 
# Generate synthetic data 
data = { 
    'CustomerID': np.arange(1, num_records + 1), 
    'Age': np.random.randint(18, 80, num_records), 
    'Gender': np.random.choice(['Male', 'Female'], num_records), 
    'ContractType': np.random.choice(['Month-to-month', 'One year', 'Two year'], 
num_records), 
    'MonthlyCharges': np.round(np.random.normal(70, 30, num_records), 2), 
 'TotalCharges': np.round(np.random.uniform(100, 8000, num_records), 2), 
    'TechSupport': np.random.choice(['Yes', 'No'], num_records), 
    'InternetService': np.random.choice(['DSL', 'Fiber optic', 'No'], num_records), 
    'Tenure': np.random.randint(1, 72, num_records), 
    'PaperlessBilling': np.random.choice(['Yes', 'No'], num_records), 
    'PaymentMethod': np.random.choice(['Electronic check', 'Mailed check', 'Bank 
transfer (automatic)', 'Credit card (automatic)'], num_records), 
    'Churn': np.random.choice(['Yes', 'No'], num_records, p=[0.2, 0.8]) 
}
# Creating derived features 
df['average_monthly_charges'] = df['TotalCharges'] / df['Tenure'] 
df['customer_lifetime_value'] = df['Tenure'] * df['MonthlyCharges']
# Introduce missing values and outliers 
df.loc[np.random.choice(df.index, size=500, replace=False), 'TotalCharges'] = 
np.nan 
df.loc[np.random.choice(df.index, size=50, replace=False), 'MonthlyCharges'] *= 
10  # Outliers
# Display the first few rows 
df.head()
# Display the first few rows 
df.head()
numeric_df = df.select_dtypes(include=[np.number]) 
# Summary statistics 
print(df.describe())
# Distribution of categorical variables 
sns.countplot(data=df, x='Gender') 
plt.show() 
 
sns.countplot(data=df, x='ContractType') 
plt.show()
# Relationship between MonthlyCharges and Churn 
sns.boxplot(data=df, x='Churn', y='MonthlyCharges') 
plt.show()
# Correlation heatmap 
plt.figure(figsize=(10, 8)) 
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm') 
plt.show()
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import LabelEncoder, StandardScaler 
 
# Handling missing values 
df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median()) 
# Encoding categorical variables 
label_encoders = {} 
for column in ['Gender', 'ContractType', 'TechSupport', 'InternetService', 
'PaperlessBilling', 'PaymentMethod', 'Churn']: 
    le = LabelEncoder() 
    df[column] = le.fit_transform(df[column]) 
    label_encoders[column] = le
# Feature scaling 
scaler = StandardScaler() 
df[['MonthlyCharges', 'TotalCharges', 'average_monthly_charges', 
'customer_lifetime_value']] = scaler.fit_transform( 
    df[['MonthlyCharges', 'TotalCharges', 'average_monthly_charges', 
'customer_lifetime_value']] 
)
# Splitting the dataset 
X = df.drop(['CustomerID', 'Churn'], axis=1) 
y = df['Churn'] 
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
random_state=42) 
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, 
random_state=42)
# Additional feature engineering if needed 
X_train['charges_per_tenure'] = X_train['TotalCharges'] / X_train['Tenure'] 
X_val['charges_per_tenure'] = X_val['TotalCharges'] / X_val['Tenure'] 
X_test['charges_per_tenure'] = X_test['TotalCharges'] / X_test['Tenure'] 
# Drop any unnecessary or highly correlated features based on EDA 
X_train.drop(['customer_lifetime_value'], axis=1, inplace=True) 
X_val.drop(['customer_lifetime_value'], axis=1, inplace=True) 
X_test.drop(['customer_lifetime_value'], axis=1, inplace=True) 
from sklearn.linear_model import LogisticRegression 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score, precision_score, recall_score, 
f1_score, roc_auc_score, confusion_matrix, classification_report,roc_curve 
from sklearn.pipeline import make_pipeline 
from sklearn.model_selection import GridSearchCV 
from sklearn.preprocessing import StandardScaler 
# Standardize features 
scaler = StandardScaler() 
X_train_scaled = scaler.fit_transform(X_train) 
X_test_scaled = scaler.transform(X_test)
# Decision Tree 
dt = DecisionTreeClassifier(max_depth=5, random_state=42) 
dt.fit(X_train, y_train) 
y_pred_dt = dt.predict(X_val) 
# Evaluate models
def evaluate_model(y_true, y_pred): 
    print(f"Accuracy: {accuracy_score(y_true, y_pred)}") 
    print(f"Precision: {precision_score(y_true, y_pred,average='binary', 
zero_division=1)}") 
    print(f"Recall: {recall_score(y_true, y_pred)}") 
    print(f"F1 Score: {f1_score(y_true, y_pred)}") 
    print(f"ROC AUC: {roc_auc_score(y_true, y_pred)}") 
    print(f"Classification Report:\n{classification_report(y_true, y_pred)}") 
    print(classification_report(y_true, y_pred, zero_division=1))
print("\nDecision Tree:") 
evaluate_model(y_val, y_pred_dt)
# Hyperparameter Tuning for Decision Tree 
param_grid = {'max_depth': [3, 5, 7, 10], 'min_samples_split': [10, 20, 30]} 
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy') 
grid_search.fit(X_train, y_train) 
cm = confusion_matrix(y_val, y_pred) 
print(cm) 
print(f"ROC AUC: {roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]):.4f}\n")
sns.heatmap(cm, annot=True, fmt='g') 
plt.xlabel('Predicted') 
plt.ylabel('Actual') 
plt.show() 
lr = LogisticRegression(max_iter=5000) 
lr.fit(X_train,y_train)
accuracy_lr = lr.score(X_test,y_test) 
print("Logistic Regression accuracy is :",accuracy_lr) 
predictdt_y = lr.predict(X_test) 
print(confusion_matrix(y_test,predictdt_y)) 
cm = confusion_matrix(y_test, predictdt_y) 
# Plot the confusion matrix 
sns.heatmap(cm, annot=True, fmt='g') 
plt.xlabel('Predicted') 
plt.ylabel('Actual')  
plt.show() 
y_pred_prob = lr.predict_proba(X_test)[:,1] 
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
plt.plot([0, 1], [0, 1], 'k--' ) 
plt.plot(fpr, tpr, label='Logistic Regression',color = "r") 
plt.xlabel('False Positive Rate') 
plt.ylabel('True Positive Rate') 
plt.title('Logistic Regression ROC Curve',fontsize=16) 
plt.show();
import xgboost as xgb 
# Initialize and train the XGBoost model 
xgb_model = xgb.XGBClassifier(eval_metric='logloss') 
xgb_model.fit(X_train, y_train) 
 
# Make predictions on the test set 
xgb_pred = xgb_model.predict(X_test) 
 
# Evaluate the model 
accuracy = accuracy_score(y_test, xgb_pred) 
precision = precision_score(y_test, xgb_pred) 
recall = recall_score(y_test, xgb_pred) 
f1 = f1_score(y_test, xgb_pred) 
 
print(f"Accuracy: {accuracy:.2f}") 
print(f"Precision: {precision:.2f}") 
print(f"Recall: {recall:.2f}") 
print(f"F1 Score: {f1:.2f}") 
# Confusion matrix 
cm = confusion_matrix(y_test, rf_pred) 
plt.figure(figsize=(8, 6)) 
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues') 
plt.xlabel('Predicted labels') 
plt.ylabel('True labels')
plt.title('Confusion Matrix - Random Forest') 
plt.show()
# Feature Importance 
importances = rf_model.feature_importances_ 
features = X_train.columns
from sklearn.ensemble import RandomForestClassifier 
# Initialize and train the Random Forest model 
rf_model = RandomForestClassifier(random_state=42) 
rf_model.fit(X_train, y_train) 
 
# Make predictions on the test set 
rf_pred = rf_model.predict(X_test) 
 
# Evaluate the model 
accuracy = accuracy_score(y_test, rf_pred) 
precision = precision_score(y_test, rf_pred) 
recall = recall_score(y_test, rf_pred) 
f1 = f1_score(y_test, rf_pred) 
 
print(f"Accuracy: {accuracy:.2f}") 
print(f"Precision: {precision:.2f}") 
print(f"Recall: {recall:.2f}") 
print(f"F1 Score: {f1:.2f}") 
# Confusion matrix 
cm = confusion_matrix(y_test, rf_pred) 
plt.figure(figsize=(8, 6)) 
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues') 
plt.xlabel('Predicted labels') 
plt.ylabel('True labels') 
plt.title('Confusion Matrix - Random Forest') 
plt.show()
# Feature Importance 
importances = rf_model.feature_importances_ 
features = X_train.columns 
 
plt.figure(figsize=(10, 6)) 
plt.barh(features, importances, color='skyblue') 
plt.xlabel('Feature Importance') 
plt.title('Feature Importance - Random Forest') 
plt.show() 
from sklearn.ensemble import GradientBoostingClassifier  # Import the class 
from sklearn.metrics import accuracy_score 
 
gb = GradientBoostingClassifier() 
gb.fit(X_train, y_train) 
gb_pred = gb.predict(X_test) 
print("Gradient Boosting Classifier", accuracy_score(y_test, gb_pred)) 
print(classification_report(y_test, gb_pred)) 
plt.figure(figsize=(4,3))
 sns.heatmap(confusion_matrix(y_test, gb_pred), 
                annot=True,fmt = "d",linecolor="k",linewidths=3) 
     
plt.title("Gradient Boosting Classifier Confusion Matrix",fontsize=14) 
plt.show() 
print(classification_report(y_test, gb_pred)) 
models = {'Logistic Regression': lr, 'Decision Tree': grid_search.best_estimator_} 
# Check class distribution (example with pandas) 
class_counts = y_train.value_counts() 
print(class_counts) 
# Calculate and display confusion matrix (potentially informative even with 
warnings) 
cm = confusion_matrix(y_test, predictdt_y) 
print(cm) 
 
# Consider alternative evaluation metrics if needed 
from sklearn.metrics import recall_score, f1_score, roc_auc_score 
 
recall = recall_score(y_test, predictdt_y) 
f1 = f1_score(y_test, predictdt_y) 
roc_auc = roc_auc_score(y_test, predictdt_y) 
 
print(f"ROC AUC: {roc_auc:.4f}")
# Assuming the Decision Tree performs better based on evaluation metrics 
 
# Retrain the best model on the full training set 
 
best_model = DecisionTreeClassifier(max_depth=5, random_state=42) 
best_model.fit(X_train, y_train) 
 
# Evaluate on the test set 
y_pred_test = best_model.predict(X_test) 
evaluate_model(y_test, y_pred_test) 
 
# Feature Importance 
importances = best_model.feature_importances_ 
features = X_train.columns 
importance_df = pd.DataFrame({'Feature': features, 'Importance': 
importances}).sort_values(by='Importance', ascending=False) 
print(importance_df)
# Assuming Decision Tree is the selected model 
final_model = grid_search.best_estimator_ 
 
# Final evaluation on the test set 
y_test_pred = final_model.predict(X_test) 
print("Final Model Evaluation on Test Set") 
print(classification_report(y_test, y_test_pred)) 
print("Confusion Matrix:") 
print(confusion_matrix(y_test, y_test_pred)) 
print(f"Test ROC AUC: {roc_auc_score(y_test, final_model.predict_proba(X_test)[:, 
1]):.4f}")
# Feature Importance 
importances = final_model.feature_importances_ 
feature_names = X.columns 
 
# **Correction:** Sort importances and feature names together by importance 
(descending) 
sorted_idx = importances.argsort()[::-1] 
importances_sorted = importances[sorted_idx] 
feature_names_sorted = feature_names[sorted_idx] 
 
# Plot Feature Importance 
plt.figure(figsize=(12, 8)) 
sns.barplot(x=importances_sorted, y=feature_names_sorted) 
plt.title('Feature Importance') 
plt.show()


